{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aiy_retrain_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "license"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "license"
      },
      "source": [
        "##### *Copyright 2020 Google LLC*\n",
        "*Licensed under the Apache License, Version 2.0 (the \"License\")*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "rKwqeqWBXANA"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTa3Ee15WsJ"
      },
      "source": [
        "# Retrain a classification model for the AIY Vision Kit (with TF1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaX0smDP7xQY"
      },
      "source": [
        "In this tutorial, we'll use TensorFlow to retrain an image classification model (MobileNet) with a flowers dataset, and compile it into the TensorFlow format that's compatible with the AIY Vision Bonnet (included in the AIY Vision Kit).\n",
        "\n",
        "All the code examples on this page are executable from your web browser, but you must execute them in order. So although you can run each code block individually, we recommend that you run everything by selecting **Runtime > Run all** in the Colab toolbar. That allows the training to get started while you read the tutorial.\n",
        "\n",
        "**Note:** Although the code on this page executes in the cloud, you **should not run this on your AIY Vision Kit**. This web app is still quite complex and the Raspberry Pi Zero cannot run it effectively (it will be very slow and you'll have a bad time). Instead, use your desktop computer to run the Colab tutorial and then transfer the files to the Raspberry Pi (as described below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viewin-badges"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/aiyprojects-raspbian/blob/aiyprojects/tutorials/vision/aiy_retrain_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<a href=\"https://github.com/google/aiyprojects-raspbian/blob/aiyprojects/tutorials/vision/aiy_retrain_classification.ipynb\" target=\"_parent\"><img src=\"https://img.shields.io/static/v1?logo=GitHub&label=&color=333333&style=flat&message=View%20on%20GitHub\" alt=\"View in GitHub\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTCYQg_be8C0"
      },
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZBTCYDICwOG"
      },
      "source": [
        "First, we need to remove the version of TensorFlow that's included with Google Colab by default, and replace it with TensorFlow 1.7, as required by the following training scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ms7m4MAPZ1R"
      },
      "source": [
        "! pip uninstall tensorflow tensorboard -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLjaKNp9inlt"
      },
      "source": [
        "! pip install -I absl-py==0.9 jupyter-client==6.1.5 tornado==5.1.0 folium==0.2.1 imgaug==0.2.5 tensorflow==1.7 tensorboard==1.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uq-26JBk4Z8"
      },
      "source": [
        "The output above  might say \"You must restart the runtime\" but you should ignore that. You **do not** need to restart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMcobPHdD8O"
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('1.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v77rlkCKW0IJ"
      },
      "source": [
        "## Prepare the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4QOy2uA3P_p"
      },
      "source": [
        "First let's download and organize the flowers dataset we'll use to retrain the model (it contains 5 flower classes).\n",
        "\n",
        "Pay attention to this part so you can reproduce it with your own images dataset. In particular, notice that the \"flower_photos\" directory contains an appropriately-named directory for each class. (If you want to retrain the model with different photos, we'll discuss this more at the very end.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9pYqideeXiI"
      },
      "source": [
        "! git clone https://github.com/googlecodelabs/tensorflow-for-poets-2\n",
        "\n",
        "%cd tensorflow-for-poets-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUJ1OqRmeusy"
      },
      "source": [
        "! curl http://download.tensorflow.org/example_images/flower_photos.tgz | tar xz -C tf_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eht5rTouezfK"
      },
      "source": [
        "! ls tf_files/flower_photos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivlTO8t7jNuG"
      },
      "source": [
        "## Retrain the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUccBS9hjZGK"
      },
      "source": [
        "First specify the input image size (this is for both width and height; the model's input expects a square image) and the depth multiplier for the MobileNet model. \n",
        "\n",
        "Based on our testing, the only variations compatible with the Vision Bonnet's ML accelerator are the following:\n",
        "\n",
        "+ Input size = 160x160, and depth multiplier = 0.5\n",
        "+ Input size = 192x192, and depth multiplier = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry4wxq4be0yR"
      },
      "source": [
        "IMAGE_SIZE='160'\n",
        "MULTIPLIER='0.50'\n",
        "%env ARCHITECTURE=mobilenet_{MULTIPLIER}_{IMAGE_SIZE}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv6FbY2llOs9"
      },
      "source": [
        "Now start training the model with the flower photos:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCJOZvfyf6HH"
      },
      "source": [
        "! python scripts/retrain.py \\\n",
        "  --bottleneck_dir=tf_files/bottlenecks \\\n",
        "  --how_many_training_steps=500 \\\n",
        "  --model_dir=tf_files/models/ \\\n",
        "  --summaries_dir=tf_files/training_summaries/$ARCHITECTURE \\\n",
        "  --output_graph=tf_files/retrained_graph.pb \\\n",
        "  --output_labels=tf_files/retrained_labels.txt \\\n",
        "  --architecture=$ARCHITECTURE \\\n",
        "  --image_dir=tf_files/flower_photos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpD2eEErpgiY"
      },
      "source": [
        "## Compile the model for the Vision Kit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhOzAdzF3Dyk"
      },
      "source": [
        "The training script above creates a TensorFlow model that you can run on a CPU, but we want to run this on the AIY Vision Bonnet's ML accelerator (the Myriad 2450). So we need to compile the model for that chip.\n",
        "\n",
        "First download the Vision Bonnet model compiler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ZpWgrk21Ad"
      },
      "source": [
        "! curl -LO https://dl.google.com/dl/aiyprojects/vision/bonnet_model_compiler_latest.tgz\t\n",
        "\n",
        "! tar -xzf bonnet_model_compiler_latest.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtPcYiER3Ymp"
      },
      "source": [
        "Then compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV5fLSQlh4qf"
      },
      "source": [
        "! ./bonnet_model_compiler.par \\\n",
        "  --frozen_graph_path=tf_files/retrained_graph.pb \\\n",
        "  --output_graph_path=tf_files/retrained_graph.binaryproto \\\n",
        "  --input_tensor_name=input \\\n",
        "  --output_tensor_names=final_result \\\n",
        "  --input_tensor_size=160 \\\n",
        "  --debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R8JMQc1MMm5"
      },
      "source": [
        "Don't worry if you see an error like this:\n",
        "\n",
        "```\n",
        "Check failed: toco::port::file::GetContents(FLAGS_frozen_graph_path, &frozen_graph_contents, toco::port::file::Options())\n",
        "```\n",
        "\n",
        "Just click the Play button in the above code to run it again. It should work this time.\n",
        "\n",
        "That's it. Your retrained model is ready to run on the Vision Kit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi9-Voc8A7VK"
      },
      "source": [
        "## Download the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiugMm-jBbWl"
      },
      "source": [
        "\n",
        "The compiled model is saved into this Colab runtime's temporary storage. You can download it to your computer like this:\n",
        "\n",
        "1. Open the **Files** tab in the left panel.\n",
        "2. Expand the **tensorflow-for-poets-2** folder and then the **tf_files** folder.\n",
        "3. Right-click on `retrained_graph.binaryproto` and select **Download**.\n",
        "4. Also download `retrained_labels.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igfS_f_mRvnR"
      },
      "source": [
        "## Copy the files to the Raspberry Pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37FMLRUpR1K5"
      },
      "source": [
        "First you need to get the Raspberry Pi's IP address so you can transfer the model files using SSH:\n",
        "\n",
        "1. Log into the Raspberry Pi and follow the [instructions to enable remote SSH access](https://www.raspberrypi.org/documentation/remote-access/ssh/).\n",
        "2. On the Pi, open the terminal and run this command:\n",
        "\n",
        "   ```\n",
        "   hostname -I\n",
        "   ```\n",
        "\n",
        "3. Write down the first IP address that's printed. For example, `192.168.86.24` (yours might be different).\n",
        "4. Also make sure the Raspberry Pi is [connected to Wi-Fi](https://www.raspberrypi.org/documentation/configuration/wireless/desktop.md) (must be on the same Wi-Fi as your desktop computer).\n",
        "\n",
        "Now go back to your desktop computer and transfer the files:\n",
        "1. Open a terminal and navigate to where the model files were downloaded (usually the `$HOME/Downloads` directory):\n",
        "\n",
        "  ```\n",
        "  cd ~/Downloads\n",
        "  ```\n",
        "\n",
        "2. Using the Raspberry Pi's IP address, transfer the files using `scp`. For example:\n",
        "\n",
        "  ```\n",
        "  scp retrained_*  pi@192.168.86.24:/home/pi\n",
        "  ```\n",
        "\n",
        "3. If it says the authenticity of the host can't be established and asks \"are you sure you want to continue?\", type \"yes\" and press Enter.\n",
        "\n",
        "4. When prompted, enter the password for your Pi. By default, the password is \"raspberry\" but you might have changed this already.\n",
        "\n",
        "If successful, your terminal prints the files that it transferred:\n",
        "\n",
        "```\n",
        "retrained_graph.binaryproto      100% 2618KB   2.0MB/s   00:01    \n",
        "retrained_labels.txt             100%   40     6.0KB/s   00:00 \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TZTwG7nhm0C"
      },
      "source": [
        "## Run the model on the Vision Kit\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwywT4ZpQjLf"
      },
      "source": [
        "1.  Log into the Raspberry Pi either via the desktop or with SSH from your desktop. If you use SSH then you can easily copy-paste the next command to run on the Pi. For example, you can SSH to the Pi like this (using the same IP address you got from above):\n",
        "\n",
        "  ```\n",
        "  ssh pi@192.168.86.24\n",
        "  ```\n",
        "\n",
        "2.  Double check that the transferred files are where you expect them:\n",
        "\n",
        "  ```\n",
        "  cd ~ && ls\n",
        "  ```\n",
        "\n",
        "  You should see `retrained_graph.binaryproto` and `retrained_labels.txt`.\n",
        "\n",
        "3.  Make sure there are no other programs currently using the Pi Camera.\n",
        "\n",
        "  For example, by default, the Vision Kit automatically runs the Joy Detector at startup. You can stop it with this command on the Pi:\n",
        "\n",
        "  ```\n",
        "  sudo systemctl stop joy_detection_demo\n",
        "  ```\n",
        "\n",
        "4.  Now run this command from the Raspberry Pi shell to test the retrained model that you transferred to the board:\n",
        "\n",
        "  ```\n",
        "  ~/AIY-projects-python/src/examples/vision/mobilenet_based_classifier.py \\\n",
        "      --model_path ~/retrained_graph.binaryproto \\\n",
        "      --label_path ~/retrained_labels.txt \\\n",
        "      --input_height 160 \\\n",
        "      --input_width 160 \\\n",
        "      --input_layer input \\\n",
        "      --output_layer final_result \\\n",
        "      --preview\n",
        "  ```\n",
        "\n",
        "It takes about 10 seconds for the model to load and start printing results. \n",
        "\n",
        "It will print the top classifications that the model detects based on what the camera sees. If you have a monitor connected to the Raspberry Pi, it also displays the camera preview with the classification printed at the top.\n",
        "\n",
        "Try holding up images of different flowers that were included in the training dataset (daisies, dandelions, roses, sunflowers, and tulips). You can search for these flowers online and point the Vision Kit at the photos on your screen to see the predictions.\n",
        "\n",
        "The predictions should be pretty good, but this was just a basic training example. With more experience with TensorFlow and some modifications to the training parameters used above, you can create a much more accurate model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S19bdVmmdEhq"
      },
      "source": [
        "## Recap and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4sJBQRdGZg"
      },
      "source": [
        "In this tutorial Colab, you downloaded a bunch of flower photos and used them to retrain the MobileNet V1 image classification model.\n",
        "\n",
        "To perform the training, we used the `retrain.py` Python script, which can quickly retrain an image classificaiton model using a relatively-small number of sample images. It does so by reusing most of the model's pre-trained weights, but replacing the top layers that perform the final classification and training new weights for that part of the model only.\n",
        "\n",
        "To learn more about how this training script works, look at the source code for the [`retrain.py` script](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuhvlj7ore5x"
      },
      "source": [
        "### Train with your own images\n",
        "\n",
        "Using this script and all the code above, try using your own image dataset to train the model to identify different objects. To get good results, you should provide a few hundred sample images for each object you want to recognize. \n",
        "\n",
        "Just put the images for each object class into a folder that's named corresponding to that object (for example, see how the [flowers dataset](http://download.tensorflow.org/example_images/flower_photos.tgz) is organized). \n",
        "\n",
        "Also be sure your images are resized appropriately; although it's not necessary that they match the model's input size exactly, they should not be very large resolution (again, inspect the flowers dataset for example image sizes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onz7Cv-Hrg1L"
      },
      "source": [
        "### Build a program to run the model\n",
        "\n",
        "A good place to start learning this part is to read the source code for the [`mobilenet_based_classifier.py` script](https://github.com/google/aiyprojects-raspbian/blob/aiyprojects/src/examples/vision/mobilenet_based_classifier.py) (used above to run the model).\n",
        "\n",
        "You might want to just copy that code and make whatever changes you want so the program does what you want, such as turn on an LED or make a noise when the camera detects a specific object (try turning on the LED if the returned results includes a specific label ID with a probability score higher than 0.5).\n",
        "\n",
        "To execute an inference with each camera frame, this code uses the [`aiy.vision.inference`](https://aiyprojects.readthedocs.io/en/latest/aiy.vision.inference.html) API. So check out that API reference to better understand the example code and see what else you can do with the AIY Vision API."
      ]
    }
  ]
}
